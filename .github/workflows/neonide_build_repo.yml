name: NeonIDE - Build & publish APT repo (aarch64)

on:
  push:
    branches:
    - master
    - main
    - dev
    - 'dev/**'
    paths:
    - 'packages/**'
    - 'root-packages/**'
    - 'x11-packages/**'
  schedule:
    - cron: '30 * * * *'
  workflow_dispatch:
    inputs:
      packages:
        description: "Optional: explicit list of source package recipes to build (names under ./packages). Separate by space/comma/newline. Example: 'moria jsoncpp'. If empty, auto-selects packages."
        required: false
        default: ""
      batch_size:
        description: "Max number of source packages to build per run (total across all shards). Ignored when 'packages' is provided."
        required: false
        default: "80"
      shard_count:
        description: "Number of parallel shards (jobs) to build in parallel"
        required: false
        default: "16"
      force_rebuild:
        description: "Rebuild even if package already exists in current Packages index"
        required: false
        type: boolean
        default: false

permissions: {}

concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: false

jobs:
  plan:
    name: Plan shards
    runs-on: ubuntu-latest
    permissions:
      contents: read
    outputs:
      matrix: ${{ steps.set-matrix.outputs.matrix }}
    steps:
      - uses: actions/checkout@v5
        with:
          fetch-depth: 1

      - id: set-matrix
        name: Build matrix JSON
        run: |
          set -euo pipefail

          # `inputs.*` is only populated for workflow_dispatch. For push builds it is
          # an empty string, so provide the workflow_dispatch default here too.
          SHARDS='${{ inputs.shard_count }}'
          if [ -z "$SHARDS" ]; then
            SHARDS=16
          fi
          if ! [[ "$SHARDS" =~ ^[0-9]+$ ]]; then
            echo "::error ::shard_count must be an integer, got: '$SHARDS'"
            exit 1
          fi
          if [ "$SHARDS" -lt 1 ] || [ "$SHARDS" -gt 16 ]; then
            echo "::error ::shard_count must be within 1..16, got: '$SHARDS'"
            exit 1
          fi

          # If a manual package list was provided, reduce shards to at most the number
          # of requested packages. This avoids lots of empty shard jobs and makes
          # it easier to reason about distribution (e.g. 3 packages => 3 shards).
          PACKAGES_INPUT='${{ inputs.packages }}'
          if [ -n "${PACKAGES_INPUT//[[:space:]]/}" ]; then
            mapfile -t REQUESTED < <(
              printf '%s\n' "$PACKAGES_INPUT" \
                | tr ',\t' '\n' \
                | sed -E 's/#.*$//' \
                | tr ' ' '\n' \
                | sed '/^$/d' \
                | sort -u
            )
            req_count="${#REQUESTED[@]}"
            if [ "$req_count" -gt 0 ]; then
              if [ "$req_count" -lt "$SHARDS" ]; then
                echo "INFO: packages input provided ($req_count requested); reducing shard_count from $SHARDS to $req_count."
                SHARDS="$req_count"
              fi
            fi
          fi

          json='{"include":['
          for ((i=0;i<SHARDS;i++)); do
            if [ "$i" -gt 0 ]; then json+=','; fi
            json+="{\"shard\":$i,\"shards\":$SHARDS}"
          done
          json+=']}'

          echo "matrix=$json" >> "$GITHUB_OUTPUT"

  build:
    name: Build debs (shard ${{ matrix.shard }})
    needs: plan
    runs-on: ubuntu-latest
    permissions:
      contents: read
    strategy:
      fail-fast: false
      matrix: ${{ fromJSON(needs.plan.outputs.matrix) }}

    steps:
      - name: Checkout termux-packages
        uses: actions/checkout@v5
        with:
          fetch-depth: 1

      # Prepare output directories.
      # We intentionally do NOT create placeholder files: if a shard produces no .debs,
      # we skip uploading the deb artifact (but still upload logs).
      - name: Prepare shard output directories
        run: |
          set -euo pipefail
          mkdir -p prebuilt-debs logs

      - name: Prepare TERMUX_TOPDIR mount on runner
        run: |
          set -euo pipefail
          sudo mkdir -p /mnt/termux-topdir
          sudo chown -R "$USER:$USER" /mnt/termux-topdir

      # Cache only the high-value Termux build caches (NOT full build dirs).
      # This reduces repeated downloads (tarballs, etc.) across workflow runs.
      - name: Cache Termux build caches
        uses: actions/cache@v4
        with:
          path: |
            /mnt/termux-topdir/_cache
            /mnt/termux-topdir/.built-packages
            /mnt/termux-topdir/python*-crossenv-prefix-*
          key: neonide-termux-topdir-aarch64-v1-${{ github.ref_name }}-${{ github.sha }}
          restore-keys: |
            neonide-termux-topdir-aarch64-v1-${{ github.ref_name }}-
            neonide-termux-topdir-aarch64-v1-

      - name: Fetch current published Packages index (sparse shallow clone)
        run: |
          set -euo pipefail
          rm -rf pages-repo

          # Public clone for read-only (avoid needing PAT here).
          git clone --depth 1 --filter=blob:none --sparse https://github.com/Mart-01-oss/pages.git pages-repo
          (
            cd pages-repo
            # sparse-checkout uses cone mode by default; patterns must be directories.
            # Checkout the directory that contains Packages.
            git sparse-checkout set dists/stable/main/binary-aarch64
          )

      - id: select
        name: Select packages for this shard
        run: |
          set -euo pipefail

          # `inputs.*` is only populated for workflow_dispatch. For push builds it is
          # an empty string, so fall back to the workflow_dispatch defaults.
          # Use the shard count produced by the plan job (may be reduced when
          # manual 'packages' input is provided).
          SHARDS='${{ matrix.shards }}'
          SHARD='${{ matrix.shard }}'
          BATCH_SIZE='${{ inputs.batch_size }}'
          FORCE_REBUILD='${{ inputs.force_rebuild && 'true' || 'false' }}'

          [ -z "$SHARDS" ] && SHARDS=16
          [ -z "$BATCH_SIZE" ] && BATCH_SIZE=80

          if ! [[ "$BATCH_SIZE" =~ ^[0-9]+$ ]] || [ "$BATCH_SIZE" -lt 1 ]; then
            echo "::error ::batch_size must be a positive integer, got: '$BATCH_SIZE'"
            exit 1
          fi

          PAGES_PACKAGES="pages-repo/dists/stable/main/binary-aarch64/Packages"
          RELEASE_PACKAGES_URL="https://github.com/Mart-01-oss/pages/releases/download/Package/Packages"
          RELEASE_PACKAGES_FILE="release-packages.txt"

          # Load exclude list (bootstrap/core packages) into a set
          declare -A EXCLUDE=()
          if [ -f "scripts/neonide-bootstrap-packages.txt" ]; then
            while IFS= read -r p; do
              [ -z "$p" ] && continue
              EXCLUDE["$p"]=1
            done < <(tr ' ' '\n' < scripts/neonide-bootstrap-packages.txt | sed '/^$/d')
          fi

          # Load already-published binary packages into a set (best-effort).
          # We treat presence in Packages as "already built" and skip unless FORCE_REBUILD=true.
          #
          # Sources:
          # - Pages repo Packages (small debs)
          # - GitHub Release flat repo Packages (large debs)
          declare -A PUBLISHED=()
          if [ "$FORCE_REBUILD" != "true" ]; then
            if [ -f "$PAGES_PACKAGES" ]; then
              while IFS= read -r p; do
                [ -z "$p" ] && continue
                PUBLISHED["$p"]=1
              done < <((grep -E '^Package: ' "$PAGES_PACKAGES" || true) | sed -E 's/^Package: //' | sort -u)
            fi

            # Fetch release Packages index (flat). This index is used for large debs.
            rm -f "$RELEASE_PACKAGES_FILE" || true
            if curl -LfsS "$RELEASE_PACKAGES_URL" -o "$RELEASE_PACKAGES_FILE" 2>/dev/null; then
              while IFS= read -r p; do
                [ -z "$p" ] && continue
                PUBLISHED["$p"]=1
              done < <((grep -E '^Package: ' "$RELEASE_PACKAGES_FILE" || true) | sed -E 's/^Package: //' | sort -u)
            else
              echo "INFO: Could not fetch release Packages from $RELEASE_PACKAGES_URL (ok if no large repo yet)." >&2
            fi
          fi

          # Enumerate source recipes (top-level ./packages/<name>)
          mapfile -t ALL_RECIPES < <(find packages -mindepth 1 -maxdepth 1 -type d -printf '%f\n' | sort -u)

          # Priority packages first.
          PRIORITY=(jsoncpp)
          declare -A IS_PRIORITY=()
          for p in "${PRIORITY[@]}"; do IS_PRIORITY["$p"]=1; done

          # `inputs.*` is only available for workflow_dispatch. For push builds it is empty.
          PACKAGES_INPUT='${{ inputs.packages }}'

          selected_all=()

          add_if_needed() {
            local pkg="$1"
            [ -z "$pkg" ] && return 0
            [ -n "${EXCLUDE[$pkg]:-}" ] && return 0
            if [ "$FORCE_REBUILD" != "true" ] && [ -n "${PUBLISHED[$pkg]:-}" ]; then
              return 0
            fi
            selected_all+=("$pkg")
          }

          add_if_requested() {
            local pkg="$1"
            [ -z "$pkg" ] && return 0
            if [ -n "${EXCLUDE[$pkg]:-}" ]; then
              echo "::error ::Requested package '$pkg' is in neonide-bootstrap-packages exclude list. Remove it from the request or update scripts/neonide-bootstrap-packages.txt." >&2
              exit 1
            fi
            if [ -z "${HAS_RECIPE[$pkg]:-}" ]; then
              echo "::error ::Requested package '$pkg' does not exist as a recipe directory: packages/$pkg" >&2
              exit 1
            fi
            # Respect published-skip logic unless force_rebuild=true
            if [ "$FORCE_REBUILD" != "true" ] && [ -n "${PUBLISHED[$pkg]:-}" ]; then
              echo "INFO: Requested package '$pkg' is already published; skipping (set force_rebuild=true to rebuild)." >&2
              return 0
            fi
            selected_all+=("$pkg")
          }

          # Fast lookup of existing recipes
          declare -A HAS_RECIPE=()
          for p in "${ALL_RECIPES[@]}"; do HAS_RECIPE["$p"]=1; done

          if [ -n "${PACKAGES_INPUT//[[:space:]]/}" ]; then
            echo "INFO: Using manually provided package list (workflow_dispatch input 'packages')."

            # Normalize: allow space/comma/newline separated values; allow passing 'packages/<name>' too.
            mapfile -t REQUESTED < <(
              printf '%s\n' "$PACKAGES_INPUT" \
                | tr ',\t' '\n' \
                | sed -E 's/#.*$//' \
                | tr ' ' '\n' \
                | sed '/^$/d' \
                | sort -u
            )

            if [ "${#REQUESTED[@]}" -eq 0 ]; then
              echo "::error ::Input 'packages' was provided but no valid package names were parsed."
              exit 1
            fi

            echo "INFO: Requested packages (unique): ${REQUESTED[*]}"

            for token in "${REQUESTED[@]}"; do
              pkg="${token##*/}"
              add_if_requested "$pkg"
            done

            if [ "${#selected_all[@]}" -eq 0 ]; then
              echo "::error ::All requested packages were skipped (most likely because they are already published). Set force_rebuild=true to rebuild." >&2
              exit 1
            fi

            echo "INFO: Will build (after filtering already-published unless force_rebuild=true): ${selected_all[*]}"
          else
            # Auto-selection mode (existing behavior)

            # Add priority packages (if they exist as recipes)
            for p in "${PRIORITY[@]}"; do
              if [ -n "${HAS_RECIPE[$p]:-}" ]; then
                add_if_needed "$p"
              fi
            done

            # Then add the rest in sorted order
            for p in "${ALL_RECIPES[@]}"; do
              [ -n "${IS_PRIORITY[$p]:-}" ] && continue
              add_if_needed "$p"
            done

            # Apply BATCH_SIZE across all shards (truncate before sharding)
            if [ "${#selected_all[@]}" -gt "$BATCH_SIZE" ]; then
              selected_all=("${selected_all[@]:0:$BATCH_SIZE}")
            fi
          fi

          # Shard distribution (round-robin)
          out="shard-packages.txt"
          : > "$out"
          idx=0
          for p in "${selected_all[@]}"; do
            if (( idx % SHARDS == SHARD )); then
              echo "$p" >> "$out"
            fi
            idx=$((idx+1))
          done

          count="$(wc -l < "$out" | tr -d ' ')"
          echo "count=$count" >> "$GITHUB_OUTPUT"

          if [ "$count" -eq 0 ]; then
            echo "No packages selected for shard $SHARD/$SHARDS."
            echo "has_packages=false" >> "$GITHUB_OUTPUT"
          else
            echo "Selected $count package(s) for shard $SHARD/$SHARDS (batch_size=$BATCH_SIZE, force_rebuild=$FORCE_REBUILD):"
            sed -n '1,200p' "$out"
            echo "has_packages=true" >> "$GITHUB_OUTPUT"
          fi

      # From here on, do the heavy CI setup only if we actually need to build.
      - name: Disk space (before)
        if: ${{ steps.select.outputs.has_packages == 'true' }}
        run: |
          set -euo pipefail
          df -h

      - name: Free disk space (lightweight)
        if: ${{ steps.select.outputs.has_packages == 'true' }}
        run: |
          set -euo pipefail
          sudo rm -rf /usr/share/dotnet /usr/local/lib/android /opt/ghc || true
          sudo apt-get clean || true
          sudo rm -rf /var/lib/apt/lists/* || true
          df -h

      - name: Move Docker data-root to /mnt/docker
        if: ${{ steps.select.outputs.has_packages == 'true' }}
        run: |
          set -euo pipefail
          DOCKER_ROOT="/mnt/docker"
          sudo systemctl stop docker
          sudo mkdir -p "$DOCKER_ROOT"
          if [ -d /var/lib/docker ] && [ -n "$(ls -A /var/lib/docker 2>/dev/null || true)" ]; then
            sudo rsync -aHAX --delete /var/lib/docker/ "$DOCKER_ROOT/" || true
          fi
          echo "{\"data-root\": \"$DOCKER_ROOT\"}" | sudo tee /etc/docker/daemon.json
          sudo systemctl start docker
          docker info | sed -n '1,120p'
          df -h

      - id: llvm
        name: Detect LLVM base dir inside docker builder
        if: ${{ steps.select.outputs.has_packages == 'true' }}
        env:
          TERMUX_DOCKER__HOST_TERMUX_TOPDIR: /mnt/termux-topdir
        run: |
          set -euo pipefail
          HOST_LLVM_BASE_DIR_IN_CONTAINER="$(./scripts/run-docker.sh bash -lc '
            set -e
            best=""
            for d in /usr/lib/llvm-*; do
              if [ -x "$d/bin/clang" ]; then best="$d"; fi
            done
            if [ -n "$best" ]; then echo "$best"; else echo "/usr"; fi
          ' | tail -n 1)"
          echo "HOST_LLVM_BASE_DIR_IN_CONTAINER=$HOST_LLVM_BASE_DIR_IN_CONTAINER" >> "$GITHUB_OUTPUT"
          echo "Using LLVM base dir: $HOST_LLVM_BASE_DIR_IN_CONTAINER"

      - id: build_shard
        name: Build packages for this shard
        if: ${{ steps.select.outputs.has_packages == 'true' }}
        env:
          TERMUX_DOCKER__HOST_TERMUX_TOPDIR: /mnt/termux-topdir
          HOST_LLVM_BASE_DIR_IN_CONTAINER: ${{ steps.llvm.outputs.HOST_LLVM_BASE_DIR_IN_CONTAINER }}
        run: |
          set -euo pipefail

          shard_failed=0
          : > logs/FAILED_PACKAGES.txt

          while IFS= read -r pkg; do
            [ -z "$pkg" ] && continue
            echo "============================================================"
            echo "[*] Building $pkg (shard ${{ matrix.shard }})"
            echo "============================================================"

            mkdir -p output
            rm -f output/*.deb || true

            log_path="logs/${pkg}-shard-${{ matrix.shard }}.log"
            : > "$log_path"

            # First try with -i (download deps from NeonIDE repo). If a dependency is missing
            # from the repo (common on fresh repos / after partial publishes), retry without -i
            # so dependencies can be built from source instead of downloaded.
            set +e
            set -o pipefail
            ./scripts/run-docker.sh env \
              TERMUX_HOST_LLVM_BASE_DIR="$HOST_LLVM_BASE_DIR_IN_CONTAINER" \
              BUILD_CC="$HOST_LLVM_BASE_DIR_IN_CONTAINER/bin/clang" \
              TERMUX_REPO_APP__PACKAGE_NAME="com.neonide.studio" \
              TERMUX_ALLOW_UNVERIFIED_REPOS="true" \
              TERMUX_ALLOW_REVISION_MISMATCH="true" \
              TERMUX_USE_OFFICIAL_REPO_FALLBACK="false" \
              TERMUX_RELEASE_DEB_REPO_URL="https://github.com/Mart-01-oss/pages/releases/download/Package" \
              HOME="/home/builder" \
              PKG="$pkg" \
              bash -lc 'set -euo pipefail; cd "$HOME/termux-packages"; . ./scripts/properties.sh; if [ ! -d "$NDK" ]; then echo "INFO: NDK missing in container at $NDK, running setup-android-sdk.sh"; ./scripts/setup-android-sdk.sh; fi; exec ./build-package.sh -i -a aarch64 "$PKG"' 2>&1 | tee -a "$log_path"
            rc=${PIPESTATUS[0]}

            #if [ $rc -ne 0 ]; then
              #echo "[!] Build with -i failed for $pkg (rc=$rc). Retrying without -i..." | tee -a "$log_path" >&2

              # The Termux docker builder reuses a single long-lived container (termux-package-builder).
              # If the previous build was interrupted or left processes behind, the next invocation may
              # hit the global build lock ("Another build is already running within same environment").
              # Restart the container before retrying to ensure a clean environment.
              #docker rm -f termux-package-builder >/dev/null 2>&1 || true

              #./scripts/run-docker.sh env \
                #TERMUX_HOST_LLVM_BASE_DIR="$HOST_LLVM_BASE_DIR_IN_CONTAINER" \
                #BUILD_CC="$HOST_LLVM_BASE_DIR_IN_CONTAINER/bin/clang" \
                #TERMUX_REPO_APP__PACKAGE_NAME="com.neonide.studio" \
                #TERMUX_ALLOW_UNVERIFIED_REPOS="true" \
               # TERMUX_ALLOW_REVISION_MISMATCH="true" \
                #TERMUX_USE_OFFICIAL_REPO_FALLBACK="false" \
               # TERMUX_RELEASE_DEB_REPO_URL="https://github.com/Mart-01-oss/pages/releases/download/Package" \
              #  HOME="/home/builder" \
               # PKG="$pkg" \
              #  bash -lc 'set -euo pipefail; cd "$HOME/termux-packages"; . ./scripts/properties.sh; if [ ! -d "$NDK" ]; then echo "INFO: NDK missing in container at $NDK, running setup-android-sdk.sh"; ./scripts/setup-android-sdk.sh; fi; exec ./build-package.sh -a aarch64 "$PKG"' 2>&1 | tee -a "$log_path"
             # rc=${PIPESTATUS[0]}
           # fi
            set +o pipefail
            set -e

            if [ $rc -ne 0 ]; then
              echo "[!] FAILED: $pkg (rc=$rc)" | tee -a "$log_path" >&2
              echo "$pkg" >> logs/FAILED_PACKAGES.txt
              shard_failed=1
              continue
            fi

            # Build succeeded: remove log to keep artifacts small.
            rm -f "$log_path" || true

            shopt -s nullglob
            for deb in output/*.deb; do
              base="$(basename "$deb")"
              # GitHub upload-artifact rejects filenames with ':' (and some other chars)
              # to remain filesystem-agnostic (e.g. Windows/NTFS).
              safe="${base//:/+}"
              cp -f "$deb" "prebuilt-debs/$safe"
            done
            shopt -u nullglob
          done < shard-packages.txt

          # Expose shard_failed to later steps (even if we exit non-zero).
          echo "shard_failed=$shard_failed" >> "$GITHUB_OUTPUT"

          echo "[*] Produced debs:";
          ls -lah prebuilt-debs || true

          if [ "$shard_failed" -ne 0 ]; then
            echo "::error ::One or more packages failed on shard ${{ matrix.shard }} (see FAILED_PACKAGES.txt + logs artifact)."
            exit 1
          fi

      - name: Upload built .deb artifacts (per shard)
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: prebuilt-debs-shard-${{ matrix.shard }}
          path: prebuilt-debs/*.deb
          if-no-files-found: ignore

      - name: Upload build logs (per shard, failures only)
        if: ${{ always() && steps.select.outputs.has_packages == 'true' && steps.build_shard.outputs.shard_failed == '1' }}
        uses: actions/upload-artifact@v4
        with:
          name: build-logs-shard-${{ matrix.shard }}
          path: logs
          if-no-files-found: warn

      - name: Docker cleanup (always)
        if: ${{ always() && steps.select.outputs.has_packages == 'true' }}
        run: |
          set -euo pipefail
          docker system prune -af --volumes || true
          df -h

  publish:
    name: Publish combined debs to pages repo
    needs: build
    # Always run publish after the matrix completes, even if some shards failed.
    # This allows publishing whatever debs were successfully built.
    if: ${{ always() }}
    runs-on: ubuntu-latest
    permissions:
      contents: write
    steps:
      - name: Checkout termux-packages
        uses: actions/checkout@v5
        with:
          fetch-depth: 1

      - name: Install host tools
        run: |
          set -euo pipefail
          sudo apt-get update
          sudo apt-get install -y --no-install-recommends dpkg-dev gzip ca-certificates git gnupg gh jq

      - name: Prepare release/publish directories
        run: |
          set -euo pipefail
          mkdir -p prebuilt-debs prebuilt-debs-large prebuilt-debs-small

      - name: Import repo signing key (optional)
        env:
          NEONIDE_GPG_PRIVATE_KEY_B64: ${{ secrets.NEONIDE_GPG_PRIVATE_KEY_B64 }}
          NEONIDE_GPG_KEY_ID: ${{ secrets.NEONIDE_GPG_KEY_ID }}
        run: |
          set -euo pipefail
          if [ -z "${NEONIDE_GPG_PRIVATE_KEY_B64:-}" ] || [ -z "${NEONIDE_GPG_KEY_ID:-}" ]; then
            echo "No signing key configured (NEONIDE_GPG_PRIVATE_KEY_B64/NEONIDE_GPG_KEY_ID missing). Repo will be unsigned."
            exit 0
          fi
          echo "$NEONIDE_GPG_PRIVATE_KEY_B64" | base64 -d | gpg --batch --import
          echo "Secret key is available."
          gpg --batch --list-secret-keys --keyid-format=long "$NEONIDE_GPG_KEY_ID" | sed -n '1,120p'

      - name: Checkout pages repo (shallow)
        env:
          PAGES_PAT: ${{ secrets.PAGES_PAT }}
        run: |
          set -euo pipefail
          if [ -z "${PAGES_PAT:-}" ]; then
            echo "::error ::Missing secret PAGES_PAT (needs push access to Mart-01-oss/pages.git)"
            exit 1
          fi
          rm -rf pages-repo
          git clone --depth 1 "https://x-access-token:${PAGES_PAT}@github.com/Mart-01-oss/pages.git" pages-repo

      - name: Download all shard deb artifacts
        continue-on-error: true
        uses: actions/download-artifact@v4
        with:
          pattern: prebuilt-debs-shard-*
          path: prebuilt-debs
          merge-multiple: true

      - name: Download all shard build logs (optional)
        continue-on-error: true
        uses: actions/download-artifact@v4
        with:
          pattern: build-logs-shard-*
          path: build-logs
          merge-multiple: true

      - id: debs
        name: Find & split debs (small->pages, large->release)
        run: |
          set -euo pipefail
          mkdir -p prebuilt-debs prebuilt-debs-large prebuilt-debs-small
          find prebuilt-debs -type f ! -name '*.deb' -delete || true

          total="$(find prebuilt-debs -maxdepth 2 -type f -name '*.deb' | wc -l | tr -d ' ')"
          echo "count=$total" >> "$GITHUB_OUTPUT"

          if [ "$total" -eq 0 ]; then
            echo "[*] No .deb files produced by build shards; skipping publish."
            echo "has_debs=false" >> "$GITHUB_OUTPUT"
            exit 0
          fi

          # Keep in sync with neonide_publish_prebuilt_debs.yml default (large_deb_threshold_mb)
          threshold_mb=99
          threshold_bytes=$((threshold_mb*1024*1024))

          large=0
          small=0
          while IFS= read -r f; do
            size="$(stat -c%s "$f")"
            base="$(basename "$f")"
            if [ "$size" -ge "$threshold_bytes" ]; then
              cp -f "$f" "prebuilt-debs-large/$base"
              large=$((large+1))
            else
              cp -f "$f" "prebuilt-debs-small/$base"
              small=$((small+1))
            fi
          done < <(find prebuilt-debs -maxdepth 2 -type f -name '*.deb' -print)

          echo "[*] Debs total=$total small=$small large=$large (threshold=${threshold_mb}MiB)"
          echo "large_count=$large" >> "$GITHUB_OUTPUT"
          echo "small_count=$small" >> "$GITHUB_OUTPUT"

          if [ "$small" -gt 0 ]; then
            echo "has_small=true" >> "$GITHUB_OUTPUT"
          else
            echo "has_small=false" >> "$GITHUB_OUTPUT"
          fi

          if [ "$large" -gt 0 ]; then
            echo "has_large=true" >> "$GITHUB_OUTPUT"
          else
            echo "has_large=false" >> "$GITHUB_OUTPUT"
          fi

          echo "[*] Debs (small):"; ls -lah prebuilt-debs-small || true
          echo "[*] Debs (large):"; ls -lah prebuilt-debs-large || true

          echo "has_debs=true" >> "$GITHUB_OUTPUT"

      - name: Publish large debs to GitHub Release as flat APT repo (Packages/Release)
        if: ${{ steps.debs.outputs.has_large == 'true' }}
        env:
          # Use the pages PAT since the release lives in Mart-01-oss/pages
          GH_TOKEN: ${{ secrets.PAGES_PAT }}
          DEBS_DIR: ${{ github.workspace }}/prebuilt-debs-large
          LARGE_DEB_THRESHOLD_MB: "99"
          RELEASE_REPO: Mart-01-oss/pages
          RELEASE_TAG: Package
          APT_ARCH: aarch64
          NEONIDE_GPG_KEY_ID: ${{ secrets.NEONIDE_GPG_KEY_ID }}
          NEONIDE_GPG_PASSPHRASE: ${{ secrets.NEONIDE_GPG_PASSPHRASE }}
        run: |
          set -euo pipefail
          bash ./scripts/neonide-publish-large-debs-to-release.sh

      - name: Publish debs into pages APT repo (small debs only; no large deb indexes)
        if: ${{ steps.debs.outputs.has_small == 'true' }}
        env:
          PAGES_REPO_DIR: ${{ github.workspace }}/pages-repo
          DEBS_DIR: ${{ github.workspace }}/prebuilt-debs-small
          APT_DIST: stable
          APT_COMPONENT: main
          APT_ARCH: aarch64
          FORCE_OVERWRITE: "false"
          NEONIDE_LARGE_DEB_PUBLISH_MODE: pages
          NEONIDE_LARGE_DEB_THRESHOLD_MB: "99"
          # Do not prefix relative Filename entries with "./" in Packages.
          NEONIDE_PACKAGES_PREFIX_DOTSLASH: "false"
          NEONIDE_GPG_KEY_ID: ${{ secrets.NEONIDE_GPG_KEY_ID }}
          NEONIDE_GPG_PASSPHRASE: ${{ secrets.NEONIDE_GPG_PASSPHRASE }}
        run: |
          set -euo pipefail
          bash ./scripts/neonide-publish-prebuilt-debs.sh

      - name: Commit & push pages repo changes (if any)
        if: ${{ steps.debs.outputs.has_debs == 'true' }}
        env:
          PAGES_PAT: ${{ secrets.PAGES_PAT }}
        run: |
          set -euo pipefail
          cd pages-repo
          if [ -z "$(git status --porcelain=v1)" ]; then
            echo "No changes to push."
            exit 0
          fi
          git config user.name "github-actions[bot]"
          git config user.email "41898282+github-actions[bot]@users.noreply.github.com"
          git add -A
          git commit -m "Update aarch64 repo (parallel batch build)"
          git push origin main
